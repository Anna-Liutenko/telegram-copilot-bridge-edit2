# **Архитектурный анализ и стратегия миграции для бота-переводчика T.Buddy**

---

## **Раздел 1: Деконструкция текущей реализации в Copilot Studio**

Этот раздел закладывает основу для понимания существующей системы. Будет проведен детальный анализ визуальных и логических компонентов бота, чтобы создать четкую спецификацию — необходимый первый шаг перед написанием кода.

### **1.1. Архитектура системы и диалоговый поток**

Для понимания логики работы бота необходимо проанализировать путь пользователя с момента первого взаимодействия. Точкой входа является триггер Fallback (Изображение 5), который активируется при любом «неизвестном намерении» (On Unknown Intent). Это ключевое проектное решение указывает на то, что бот в первую очередь является реактивным переводчиком, а не многофункциональным агентом. Триггер немедленно направляет пользователя к теме LanguageGate, которая содержит основную логику, видимую на подробных блок-схемах (Изображения 6, 7). Этот поток представляет собой конечный автомат, который необходимо воспроизвести.

Пользовательский сценарий выглядит следующим образом:

1. Пользователь инициирует диалог, и система, не распознав конкретного намерения, запускает Fallback-сценарий.  
2. Система предлагает пользователю настроить языки для перевода.  
3. Пользователь в свободной форме указывает желаемые языки.  
4. Система обрабатывает этот ввод, извлекает и сохраняет список языков.  
5. Последующие сообщения пользователя система воспринимает как текст для перевода.  
6. Система определяет язык исходного текста.  
7. Система последовательно переводит текст на каждый из языков, выбранных пользователем на шаге 4, и отправляет результат.

Этот диалоговый поток полностью зависит от последовательных вызовов больших языковых моделей (LLM) для выполнения каждой из ключевых операций.

### **1.2. Основной алгоритм I: Настройка нескольких языков (промпт LanguageProcessor)**

Это первое критическое взаимодействие с LLM в логике бота. Его задача — преобразовать неструктурированный пользовательский ввод в машиночитаемый формат.

* **Функция:** Промпт LanguageProcessor (Изображение 3\) выступает в роли преобразователя естественного языка в структурированные данные. Он принимает неструктурированный пользовательский ввод (например, «Я хочу использовать русский, английский и, может быть, японский») и преобразует его в строго отформатированный JSON-массив.  
* **Технические детали:** Промпт использует модель GPT-4.1 mini. Он явно инструктирует модель о формате вывода: JSON-массив объектов, каждый из которых должен содержать два ключа: code (двухбуквенный код ISO 639-1 в верхнем регистре) и name (полное английское название языка с заглавной буквы). Использование техники обучения на нескольких примерах (few-shot learning) путем предоставления явного EXAMPLE является ключевым методом для повышения надежности вывода.  
* **Неявная зависимость:** Этот шаг полагается на «мировые знания» LLM для правильного сопоставления названий языков (например, «Russian») с их кодами ISO («RU»). Любая неточность или неоднозначность в названии языка может привести к ошибке на этом этапе.

### **1.3. Основной алгоритм II: Определение исходного языка (промпт sourceLanguageCode)**

Этот алгоритм определяет язык текста, который необходимо перевести, что позволяет избежать лишних переводов (например, с английского на английский).

* **Функция:** Промпт sourceLanguageCode (Изображение 4\) выполняет определение языка. Он принимает блок текста в качестве входных данных и выводит JSON-объект, содержащий двухбуквенный код ISO 639-1.  
* **Технические детали:** Этот промпт также использует GPT-4.1 mini и является промптом без примеров (zero-shot), полностью полагаясь на встроенные возможности модели. Инструкция «Output only the two-letter code» (Выводи только двухбуквенный код) является попыткой ограничить вывод, однако, как будет показано далее, это не является полностью надежным методом для обеспечения чистоты формата.

### **1.4. Основной алгоритм III: Основной перевод (промпт FinalTranslator)**

Это заключительный и наиболее важный шаг в конвейере перевода, от которого зависит конечный результат для пользователя.

* **Функция:** Промпт FinalTranslator (Изображение 2\) выполняет сам перевод. Он принимает две переменные в качестве входных данных: textToTranslate (текст для перевода) и langCode (целевой код языка).  
* **Технические детали:** Используя GPT-4.1 mini, промпт спроектирован для чистоты вывода: «Output ONLY the translated text. Do not add the language code prefix or any other explanations» (Выводи ТОЛЬКО переведенный текст. Не добавляй префикс с кодом языка или любые другие объяснения). Это распространенная и необходимая инструкция, чтобы предотвратить добавление LLM разговорных элементов, которые могли бы повредить вывод для программного использования.

### **1.5. Неявное управление состоянием и контекстом в Copilot Studio**

Визуальные блок-схемы (Изображения 6, 7\) показывают, как переменные устанавливаются и передаются между узлами. Copilot Studio обрабатывает это управление состоянием сессии неявно, скрывая сложность от разработчика.

* **Механизм:** Когда LanguageProcessor возвращает JSON с языками, он сохраняется в переменной (например, Topic.selectedLanguages). Когда пользователь позже отправляет сообщение для перевода, логика приложения выполняет итерацию по этой сохраненной переменной. Такое сохранение состояния между обращениями пользователя является основной функцией платформы, которую необходимо будет явно спроектировать в среде Node.js.

Вся архитектура бота представляет собой каскад вызовов LLM, где структурированный вывод одного промпта становится входом для последующей логики или других промптов. Это создает цепь зависимостей, в которой сбой на любом отдельном шаге (например, невалидный JSON от LanguageProcessor) может нарушить весь поток. Таким образом, наиболее критической точкой отказа является не качество самого перевода, а структурная целостность данных, передаваемых между шагами, управляемыми LLM. Успех миграции зависит от обеспечения надежности этой передачи данных. Следовательно, приложение на Node.js не может просто выполнить API-вызов и доверять ответу. Оно должно быть спроектировано защитно, с надежной валидацией, обработкой ошибок и логикой повторных попыток, специально нацеленной на *структуру* вывода LLM, а не только на его содержание.

---

## **Раздел 2: Архитектурный план для реализации на Node.js**

Этот раздел преобразует деконструированную логику в конкретную архитектуру программного обеспечения, предоставляя шаблоны и лучшие практики для создания бота на собственном сервере.

### **2.1. Предлагаемая серверная архитектура**

* **Фреймворк:** Рекомендуется использовать Express.js как легковесный, но мощный фреймворк для создания API-эндпоинтов, которые будут взаимодействовать с вашим чат-клиентом (например, Telegram, веб\-виджет).1  
* **Модульная структура:** Приложение должно быть структурировано по модулям: модуль routes для обработки входящих сообщений, модуль services, содержащий основную логику (например, translationService.js, languageSetupService.js), и выделенный модуль llmClient для абстрагирования прямых API-вызовов к выбранному провайдеру LLM. Такое разделение ответственности критически важно для поддержки и тестирования.  
* **Управление окружением:** Безопасная обработка API-ключей имеет первостепенное значение. Будет использоваться библиотека, такая как dotenv, для управления переменными окружения (OPENAI\_API\_KEY и т.д.), что гарантирует, что ключи не будут жестко закодированы в исходном коде.2

### **2.2. Воспроизведение логики промптов с помощью прямых вызовов LLM API**

* **Абстрагирование вызовов LLM:** Вместо визуальных узлов промптов будут созданы асинхронные функции JavaScript. Библиотеки, такие как llm.js 3, или прямое использование  
  axios/fetch с SDK OpenAI/Anthropic 2 могут упростить эту задачу. Например,  
  LanguageProcessor превратится в функцию async function getLanguagesFromText(userInput).  
* **Function Calling / Tools:** Более продвинутым и надежным подходом, чем простое промптирование для получения JSON, является использование возможностей "Function Calling" или "Tool Use" современных LLM.6 Можно определить схему функции (например,  
  set\_translation\_languages с параметром-массивом для языков), и LLM вернет структурированный JSON-объект, который гарантированно будет соответствовать этой схеме. Это мощная техника для снижения проблемы надежности JSON.  
* **Пример концептуального кода:**  
  JavaScript  
  // Использование библиотеки, подобной llm.js, для простоты  
  import LLM from "@themaximalist/llm.js";

  async function getLanguagesFromText(userInput) {  
    const languageTool \= {  
      name: "set\_translation\_languages",  
      description: "Identifies and structures the languages a user wants to use.",  
      input\_schema: {  
        type: "object",  
        properties: {  
          languages: {  
            type: "array",  
            items: {  
              type: "object",  
              properties: {  
                code: { type: "string", description: "Two-letter ISO 639-1 code, uppercase" },  
                name: { type: "string", description: "Full English language name" }  
              },  
              required: \["code", "name"\]  
            }  
          }  
        },  
        required: \["languages"\]  
      }  
    };  
    const response \= await LLM(userInput, { tools: });  
    // Возвращает структурированный JSON, соответствующий схеме  
    return response.tool\_calls.input;  
  }

### **2.3. Проектирование явной системы управления состоянием**

Это наиболее важный архитектурный компонент, который необходимо воспроизвести из Copilot Studio.

* **Необходимость:** Протокол HTTP является stateless (без сохранения состояния). Когда пользователь устанавливает языки в одном запросе, а в другом отправляет сообщение для перевода, сервер должен помнить контекст.9  
* **Хранилище в памяти (для разработки):** Простой JavaScript Map или объект можно использовать для хранения данных сессии, сопоставляя уникальный идентификатор пользователя/чата с его объектом состояния (например, { selectedLanguages: \[...\] }). Это просто, но не масштабируемо и не постоянно (данные теряются при перезапуске сервера).10  
* **Постоянное хранилище (для продакшена):** Для производственной среды настоятельно рекомендуется использовать внешнее хранилище, такое как **Redis**. Оно быстрое, постоянное и хорошо справляется с параллелизмом. Будет использоваться библиотека, такая как ioredis, для подключения приложения Node.js к экземпляру Redis. Каждое входящее сообщение будет сначала инициировать поиск сессии пользователя в Redis. После обработки обновленное состояние сессии будет записано обратно в Redis.  
* **Библиотеки управления сессиями:** Фреймворки, такие как express-session, могут абстрагировать большую часть этой сложности, предоставляя объект req.session для хранения данных, специфичных для пользователя.10

Миграция фундаментально смещает ответственность за управление состоянием, надежность и безопасность с платформы (Microsoft) на разработчика. Простота визуальной блок-схемы скрывает сложное взаимодействие логики с состоянием, оркестрации API и обработки ошибок. В Copilot переменная — это просто контейнер для данных. В Node.js этот «контейнер» нуждается в «доме». Находится ли он в памяти? Если да, что произойдет при перезапуске сервера? Все сессии пользователей будут потеряны. Это заставляет рассмотреть использование постоянного хранилища данных. Какого именно? Полноценная база данных, такая как PostgreSQL, избыточна для данных сессии. Хранилище типа «ключ-значение», такое как Redis, идеально подходит. Эта линия рассуждений показывает, что кажущаяся простой задача (запоминание выбранных языков) диктует значительную часть инфраструктуры (экземпляр Redis). Таким образом, объем проекта больше, чем просто переписывание трех промптов на JavaScript. Он требует создания stateful, безопасного и надежного веб\-приложения с надлежащими инфраструктурными соображениями. Выбор стратегии управления состоянием напрямую повлияет на масштабируемость и отказоустойчивость приложения.

---

## **Раздел 3: Анализ узких мест миграции и стратегическое их устранение**

Этот раздел напрямую затрагивает основные опасения, превращая их из неизвестных угроз в управляемые инженерные проблемы с четкими решениями.

### **3.1. Критический риск: Ненадежность вывода LLM (проблема JSON)**

* **Проблема:** Хотя промпты могут запрашивать JSON, LLM все равно могут возвращать некорректно сформированный JSON (например, с висячими запятыми, одинарными кавычками вместо двойных) или оборачивать JSON в разговорный текст (например, «Вот JSON, который вы просили:...»).11 Это приведет к сбою стандартного  
  JSON.parse(), нарушая работу приложения.  
* **Стратегия устранения — многоуровневая защита:**  
  1. **Уровень 1: Продвинутый промпт-инжиниринг:** Использование наиболее надежных техник промптинга. Явно определите схему в промпте, предоставьте несколько примеров (few-shot) и проинструктируйте модель не включать никакого пояснительного текста.13 Предварительное заполнение ответа, завершая промпт строкой  
     {"languages": \[, также может сильно направить модель.12  
  2. **Уровень 2: Специфичные для модели функции:** При использовании провайдеров, таких как OpenAI, следует задействовать функцию "JSON Mode" (response\_format={ "type": "json\_object" }), которая заставляет вывод модели быть синтаксически валидным JSON.11 Это самый надежный метод, когда он доступен. Как упоминалось в п. 2.2, "Tool Use" является еще более структурированной и предпочтительной альтернативой.6  
  3. **Уровень 3: Валидация и исправление на стороне приложения:** Код на Node.js не должен доверять выводу. Необходимо реализовать блок try-catch вокруг JSON.parse(). Если он не срабатывает, не следует сразу сдаваться. Можно использовать регулярные выражения для извлечения JSON-объекта из блоков кода markdown (json... ).15 Также можно использовать специализированную библиотеку, такую как  
     json-repair, для автоматического исправления распространенных синтаксических ошибок.14  
  4. **Уровень 4: Умные повторные попытки:** Только в крайнем случае, если парсинг и исправление не удались, следует инициировать повторный вызов LLM. Необходимо реализовать стратегию экспоненциальной задержки (exponential backoff), чтобы избежать перегрузки API.

### **3.2. Узкое место производительности: Сетевая и API задержка**

* **Проблема:** Интегрированная среда Copilot Studio, вероятно, выигрывает от соединений с низкой задержкой к моделям, размещенным в Azure. Собственный сервер добавит новые источники задержки: сетевой транзит до API провайдера LLM и собственное время вывода модели (inference time). Это может сделать бота медлительным.  
* **Стратегия устранения — измерение и оптимизация:**  
  1. **Выбор модели на основе задержки:** Производительность значительно варьируется между моделями. Необходимо проанализировать бенчмарки. **Таблица 1** будет иметь здесь решающее значение.  
  2. **Потоковая передача ответа (Streaming):** Для заключительного этапа перевода не следует ждать, пока будет сгенерирован весь переведенный текст. Необходимо использовать возможности потоковой передачи API для отправки перевода по словам или предложениям.3 Это значительно улучшает  
     *воспринимаемую* производительность, так как пользователь видит немедленный отклик.  
  3. **Оптимизация цепочки вызовов:** Следует проанализировать, всегда ли необходимы все три вызова LLM. Например, можно использовать гораздо меньшую, более быструю или даже не-LLM библиотеку для определения языка, если производительность критична.

Простой вопрос «какая модель самая быстрая?» является неверным. Данные показывают различие между «временем до первого токена» (Time to First Token, TTFT) и «скоростью генерации на токен».16 Для переводчика в реальном времени низкий TTFT имеет первостепенное значение, поскольку он сигнализирует пользователю, что система работает. Задержка в одну секунду перед появлением любого текста ощущается гораздо хуже, чем немного более медленный поток слов. Следующая таблица позволяет выбрать модель, оптимизированную для этого конкретного пользовательского опыта, — нюанс, который упускается при рассмотрении только общего времени генерации.

**Таблица 1: Сравнительная задержка LLM для задач перевода и Q\&A**

| LLM-модель | Тип задачи | Время до первого токена (с) | Задержка на токен (с/токен) | Анализ и оптимальный сценарий использования |
| :---- | :---- | :---- | :---- | :---- |
| GPT-4.1 | Перевод | 0.766 | 0.014 | Медленный старт, но самая высокая скорость для длинных текстов. |
| GPT-4.1 | Q\&A | 0.615 | 0.026 | Эффективен для длинных, развернутых ответов. |
| Mistral-large | Перевод | 0.558 | 0.042 | Умеренный старт, лучше подходит для коротких предложений. |
| Mistral-large | Q\&A | 0.495 | 0.041 | Хороший баланс для коротких, интерактивных запросов. |
| Claude-3-opus | Перевод | 1.191 | 0.046 | Медленный старт, подходит для некритичных ко времени переводов. |
| Claude-3-opus | Q\&A | 1.162 | 0.049 | Значительная начальная задержка может ухудшить UX. |

Источник: Данные скомпилированы из бенчмарков, представленных в.16

### **3.3. Финансовое узкое место: Потребление токенов и контроль затрат**

* **Проблема:** Происходит переход от потенциально фиксированной стоимости платформы к переменной, основанной на использовании. Каждый вызов LLM в трехэтапном процессе потребляет токены и стоит денег. Неконтролируемое использование может привести к неожиданно высоким счетам.17  
* **Стратегия устранения — моделирование и мониторинг:**  
  1. **Моделирование затрат:** Необходимо проанализировать стоимость одного полного взаимодействия с пользователем. **Таблица 2** предоставит эту разбивку.  
  2. **Правильный подбор моделей:** Не следует использовать мощную и дорогую модель, такую как GPT-4, для простых задач. Задачи LanguageProcessor и sourceLanguageCode — это задачи классификации и извлечения. Меньшая, более дешевая и быстрая модель (например, GPT-3.5-Turbo или Claude 3 Haiku) скорее всего будет достаточной и гораздо более экономичной.20 Премиум-модель следует использовать только для заключительного шага  
     FinalTranslator, где качество имеет наибольшее значение.  
  3. **Внедрение кэширования:** Если пользователь часто переводит одну и ту же фразу, результат следует кэшировать (в Redis, вместе с состоянием сессии), чтобы избежать избыточных вызовов API.20  
  4. **Установка бюджетов и оповещений:** Необходимо использовать панель управления провайдера LLM для установки лимитов расходов и оповещений, чтобы предотвратить неконтролируемый рост затрат.

Простой просмотр страницы с ценами провайдера (например, X долларов за 1 млн токенов) вводит в заблуждение. Разработчику необходимо понимать *экономику токенов* своего конкретного приложения. Следующая таблица заставляет провести этот анализ. Она показывает, что «подготовительные» вызовы (выбор/определение языка) могут составлять значительный процент от общей стоимости, что побуждает к оптимизации, такой как использование более дешевых моделей для этих шагов. Это превращает технический показатель (токены) в бизнес-KPI (стоимость на пользователя).

**Таблица 2: Анализ предполагаемой стоимости одного взаимодействия**

| Этап обработки | Используемая модель | Примерное кол-во входных токенов | Примерное кол-во выходных токенов | Стоимость вызова (USD) |
| :---- | :---- | :---- | :---- | :---- |
| Настройка языка (ввод: 20 слов) | GPT-3.5-Turbo | \~150 | \~50 | \~$0.000105 |
| Определение источника (ввод: 50 слов) | GPT-3.5-Turbo | \~100 | \~10 | \~$0.000056 |
| Перевод текста (50 слов) | GPT-4o | \~100 | \~70 | \~$0.00155 |
| **Итоговая стоимость за один перевод** |  |  |  | **\~$0.001711** |

*Примечание: Расчеты основаны на ценах OpenAI на август 2024 года (gpt-3.5-turbo-0125: $0.50/1M входных, $1.50/1M выходных; gpt-4o: $5.00/1M входных, $15.00/1M выходных). Количество токенов включает промпт и ответ.*

### **3.4. Операционный риск: Надежность, масштабируемость и безопасность**

* **Проблема:** С решением на собственном хостинге вся ответственность за эксплуатацию ложится на разработчика. Он отвечает за время безотказной работы, безопасность, ведение журналов и масштабирование приложения по мере роста использования.17  
* **Стратегия устранения — внедрение практик DevOps:**  
  1. **Ведение журналов (Logging):** Внедрение комплексного журналирования (с использованием библиотеки, такой как winston или pino) для записи всех запросов к API, ответов и ошибок. Это неоценимо для отладки.  
  2. **Проверки работоспособности (Health Checks):** Создание эндпоинта /health в приложении Express.js, который службы мониторинга могут опрашивать, чтобы убедиться, что приложение работает.  
  3. **Контейнеризация:** Упаковка приложения Node.js в контейнер Docker. Это делает развертывание последовательным в различных средах (разработка, стейджинг, продакшен).  
  4. **Платформа для развертывания:** Развертывание контейнера на управляемой платформе, такой как AWS Elastic Beanstalk, Google App Engine или Vercel. Эти платформы берут на себя автоматическое масштабирование, балансировку нагрузки и обслуживание серверов, позволяя сосредоточиться на коде приложения.

---

## **Раздел 4: Стратегические рекомендации и итоговый синопсис**

Этот заключительный раздел синтезирует анализ в четкий, действенный план, предоставляя дорожную карту и краткое изложение основных инженерных принципов для достижения успеха.

### **4.1. Поэтапная дорожная карта миграции**

* **Этап 1: Основная логика и локальная разработка:** Создание приложения Node.js локально. Воспроизведение трех основных алгоритмов LLM с использованием прямых вызовов API. Использование хранилища сессий в памяти. Тщательное тестирование логики парсинга и исправления JSON.  
* **Этап 2: Инфраструктура и стейджинг:** Контейнеризация приложения с помощью Docker. Настройка стейджинг-среды на выбранной облачной платформе с экземпляром Redis. Тестирование постоянного управления состоянием.  
* **Этап 3: Настройка производительности и развертывание в продакшен:** Внедрение потоковой передачи ответов и кэширования. Проведение нагрузочного тестирования для понимания характеристик производительности. Развертывание в продакшен с полным журналированием и мониторингом.  
* **Этап 4: Оптимизация затрат и итерации:** После запуска внимательно отслеживать затраты на API. Экспериментировать с различными моделями для каждого шага в цепочке, чтобы найти оптимальный баланс между стоимостью, скоростью и качеством.

### **4.2. Рекомендуемые инструменты и библиотеки (npm)**

* **Веб-сервер:** express  
* **API-клиенты:** openai, @anthropic-ai/sdk или axios  
* **Управление состоянием:** ioredis, express-session  
* **Переменные окружения:** dotenv  
* **Валидация/исправление данных:** zod (для валидации схемы), json-repair  
* **Журналирование:** winston или pino

### **4.3. Итоговый синопсис**

Миграция из Copilot Studio в самодостаточное приложение на Node.js не только осуществима, но и предлагает значительные преимущества с точки зрения контроля, кастомизации и потенциальной долгосрочной экономии средств. Однако успех проекта зависит от смены парадигмы — от опоры на интегрированную платформу к внедрению явной инженерной дисциплины.

Основные проблемы заключаются не в самой логике перевода, а в «соединительной ткани», которую low-code платформа предоставляет неявно: **надежная обработка структурированных данных, явное управление состоянием и активное управление производительностью/затратами.**

Приняв защитный, многоуровневый подход к обработке выводов LLM, внедрив постоянное хранилище сессий, такое как Redis, и принимая решения о выборе модели на основе данных о задержках и стоимости, можно создать бота-переводчика, который будет более мощным, масштабируемым и экономически эффективным, чем исходный прототип. Этот отчет предоставляет архитектурный план и стратегическое руководство для успешного осуществления этого перехода.

#### **Referências citadas**

1. Building A WhatsApp Chatbots in Node.js: Step-by-Step Guide \- Kommunicate, acessado em setembro 7, 2025, [https://www.kommunicate.io/blog/build-a-whatsapp-chatbot-using-node-js/](https://www.kommunicate.io/blog/build-a-whatsapp-chatbot-using-node-js/)  
2. Build a Fullstack Chatbot in 20 minutes with React and Node.js: A Step-By-Step Guide, acessado em setembro 7, 2025, [https://medium.com/rewrite-tech/build-your-own-fullstack-chatbot-with-react-and-node-js-a-step-by-step-guide-922b392bfbf2](https://medium.com/rewrite-tech/build-your-own-fullstack-chatbot-with-react-and-node-js-a-step-by-step-guide-922b392bfbf2)  
3. themaximalist/llm.js: Universal LLM Interface \- GitHub, acessado em setembro 7, 2025, [https://github.com/themaximalist/llm.js/](https://github.com/themaximalist/llm.js/)  
4. Integrating a chatbot into your Nodejs API using Dialogflow \- DEV Community, acessado em setembro 7, 2025, [https://dev.to/realsteveig/integrating-a-chatbot-into-your-nodejs-api-using-dialogflow-1dpn](https://dev.to/realsteveig/integrating-a-chatbot-into-your-nodejs-api-using-dialogflow-1dpn)  
5. jamesmurdza/llm-api-examples \- GitHub, acessado em setembro 7, 2025, [https://github.com/jamesmurdza/llm-api-examples](https://github.com/jamesmurdza/llm-api-examples)  
6. Function calling with the Gemini API | Google AI for Developers, acessado em setembro 7, 2025, [https://ai.google.dev/gemini-api/docs/function-calling](https://ai.google.dev/gemini-api/docs/function-calling)  
7. Using the function calling tool with Node.js and LLMs \- Red Hat Developer, acessado em setembro 7, 2025, [https://developers.redhat.com/learning/learn:diving-deeper-large-language-models-and-nodejs/resource/resources:using-function-calling-tool-nodejs-and-llms](https://developers.redhat.com/learning/learn:diving-deeper-large-language-models-and-nodejs/resource/resources:using-function-calling-tool-nodejs-and-llms)  
8. Function Calling with LLMs \- Prompt Engineering Guide, acessado em setembro 7, 2025, [https://www.promptingguide.ai/applications/function\_calling](https://www.promptingguide.ai/applications/function_calling)  
9. Building a Chatbot with React, Node.js, and OpenAI: A Step-by-Step Guide, acessado em setembro 7, 2025, [https://complereinfosystem.com/build-chatbot-react-nodejs-openai-guide](https://complereinfosystem.com/build-chatbot-react-nodejs-openai-guide)  
10. Node.js State Management for Chatbots – WhatsApp Guide ..., acessado em setembro 7, 2025, [https://green-api.com/en/docs/chatbots/nodejs/state/](https://green-api.com/en/docs/chatbots/nodejs/state/)  
11. How to extract JSON from LLM response \- writtenbykaushal, acessado em setembro 7, 2025, [https://writtenbykaushal.hashnode.dev/how-to-extract-json-from-llm-response](https://writtenbykaushal.hashnode.dev/how-to-extract-json-from-llm-response)  
12. Make JSON output more likely \- API \- OpenAI Developer Community, acessado em setembro 7, 2025, [https://community.openai.com/t/make-json-output-more-likely/718590](https://community.openai.com/t/make-json-output-more-likely/718590)  
13. How to get 100% valid JSON answers? \- Prompting \- OpenAI Developer Community, acessado em setembro 7, 2025, [https://community.openai.com/t/how-to-get-100-valid-json-answers/554379](https://community.openai.com/t/how-to-get-100-valid-json-answers/554379)  
14. Crafting Structured {JSON} Responses: Ensuring Consistent Output ..., acessado em setembro 7, 2025, [https://dev.to/rishabdugar/crafting-structured-json-responses-ensuring-consistent-output-from-any-llm-l9h](https://dev.to/rishabdugar/crafting-structured-json-responses-ensuring-consistent-output-from-any-llm-l9h)  
15. I want LLM to return output in JSON format without giving it a schema : r/LangChain \- Reddit, acessado em setembro 7, 2025, [https://www.reddit.com/r/LangChain/comments/1fy1yjq/i\_want\_llm\_to\_return\_output\_in\_json\_format/](https://www.reddit.com/r/LangChain/comments/1fy1yjq/i_want_llm_to_return_output_in_json_format/)  
16. LLM Latency Benchmark by Use Cases \- Research AIMultiple, acessado em setembro 7, 2025, [https://research.aimultiple.com/llm-latency-benchmark/](https://research.aimultiple.com/llm-latency-benchmark/)  
17. On-Prem vs Cloud: LLM Cost Breakdown | newline \- Fullstack.io, acessado em setembro 7, 2025, [https://www.newline.co/@zaoyang/on-prem-vs-cloud-llm-cost-breakdown--4a7b0926](https://www.newline.co/@zaoyang/on-prem-vs-cloud-llm-cost-breakdown--4a7b0926)  
18. LLM Service vs LLM Self-Hosting — Which Is Right for You? \- Database Mart, acessado em setembro 7, 2025, [https://www.databasemart.com/blog/llm-service-vs-llm-self-hosting](https://www.databasemart.com/blog/llm-service-vs-llm-self-hosting)  
19. Cost Analysis of deploying LLMs: A comparative Study between Cloud Managed, Self-Hosted and 3rd Party LLMs | by Hugo Debes | Artefact Engineering and Data Science | Medium, acessado em setembro 7, 2025, [https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca](https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca)  
20. Serverless vs. Dedicated LLM Deployments: A Cost-Benefit Analysis, acessado em setembro 7, 2025, [https://www.bentoml.com/blog/serverless-vs-dedicated-llm-deployments](https://www.bentoml.com/blog/serverless-vs-dedicated-llm-deployments)  
21. Self Host LLM vs Api LLM : r/AI\_Agents \- Reddit, acessado em setembro 7, 2025, [https://www.reddit.com/r/AI\_Agents/comments/1kpt89v/self\_host\_llm\_vs\_api\_llm/](https://www.reddit.com/r/AI_Agents/comments/1kpt89v/self_host_llm_vs_api_llm/)